{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenisation"
      ],
      "metadata": {
        "id": "SLvdiT1XPXt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def segment_text(input_text):\n",
        "    token_pattern = r\"\"\"\n",
        "    (?:[A-Za-z]\\.){2,}[A-Z]\n",
        "    | \\w+(?:-\\w+)+\n",
        "    | \\b([A-Za-z]+)(n't|'s|'ll|'em|'ve|'re|'d)\\b\n",
        "    | \\b\\w+\\b\n",
        "    | [.,!?;\"()\\[\\]{}<>]\n",
        "    \"\"\"\n",
        "\n",
        "    segments = []  # List to store tokens\n",
        "\n",
        "    for match in re.finditer(token_pattern, input_text, flags=re.VERBOSE):\n",
        "        if match.group(1):\n",
        "            segments.extend([match.group(1), match.group(2)])\n",
        "        else:\n",
        "            segments.append(match.group(0))\n",
        "\n",
        "    return segments\n",
        "\n",
        "text = \"isn't\"\n",
        "unique_segments = set(segment_text(text))\n",
        "print(unique_segments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlZcs6gKO4cl",
        "outputId": "daaccc29-3e1e-416d-da63-b4121468c21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"n't\", 'is'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Is Plural - FST"
      ],
      "metadata": {
        "id": "0cjib4rCP63c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_plural_noun_accepted_fsa(word):\n",
        "    if len(word) < 2 or word[-1] != 's':\n",
        "        return False\n",
        "\n",
        "    word = word[::-1]\n",
        "    state = 'S1'\n",
        "\n",
        "    for char in word[1:]:\n",
        "        if state == 'S1':\n",
        "            if char == 'y':\n",
        "                state = 'S2'\n",
        "            elif char == 'e':\n",
        "                state = 'S3'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S2':\n",
        "            if char in 'aeiou':\n",
        "                state = 'S5'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S3':\n",
        "            if char == 'i':\n",
        "                state = 'S4'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S4':\n",
        "            if char.isalpha() and char not in 'aeiou':\n",
        "                state = 'S6'\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 'S5':\n",
        "            continue\n",
        "        elif state == 'S6':\n",
        "            continue\n",
        "\n",
        "    return True\n",
        "\n",
        "test_words = ['boys', 'toys', 'ponies', 'skies', 'puppies', 'boies', 'toies', 'ponys', 'carries', 'daisies']\n",
        "results = {word: is_plural_noun_accepted_fsa(word) for word in test_words}\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQVy2FOmP9DR",
        "outputId": "98c6307d-2b1b-4269-b629-758007075f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'boys': True, 'toys': True, 'ponies': True, 'skies': True, 'puppies': True, 'boies': False, 'toies': False, 'ponys': False, 'carries': True, 'daisies': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pluralising"
      ],
      "metadata": {
        "id": "MHSEniwDQH8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pluralize_word_fst(word):\n",
        "    state = 'START'\n",
        "    result = word\n",
        "\n",
        "    if word.endswith(\"^s#\"):\n",
        "        if word.endswith(\"x^s#\"):\n",
        "            state = 'ADD_ES'\n",
        "        elif word.endswith(\"s^s#\"):\n",
        "            state = 'ADD_ES'\n",
        "        elif word.endswith(\"z^s#\"):\n",
        "            state = 'ADD_ES'\n",
        "        else:\n",
        "            state = 'ADD_S'\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "    if state == 'ADD_ES':\n",
        "        result = word.replace(\"^s#\", \"es\")\n",
        "    elif state == 'ADD_S':\n",
        "        result = word.replace(\"^s#\", \"s\")\n",
        "\n",
        "    return result\n",
        "\n",
        "test_cases = [\"fox^s#\", \"boy^s#\", \"bus^s#\", \"quiz^s#\", \"dog^s#\"]\n",
        "results = {word: pluralize_word_fst(word) for word in test_cases}\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJftrBFElk56",
        "outputId": "9b77e469-e46f-4e1e-aa66-abf308e269e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fox^s#': 'foxes', 'boy^s#': 'boys', 'bus^s#': 'buses', 'quiz^s#': 'quizes', 'dog^s#': 'dogs'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimum Edit Distance"
      ],
      "metadata": {
        "id": "0MW5XZqdK0kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EditDistance:\n",
        "    def __init__(self):\n",
        "        self.INSERT_COST = 1\n",
        "        self.DELETE_COST = 1\n",
        "        self.REPLACE_COST = 2\n",
        "\n",
        "    def minimum_edit_distance(self, source: str, target: str):\n",
        "        \"\"\"\n",
        "        Calculate minimum edit distance between source and target strings.\n",
        "        Returns the distance and the operations needed.\n",
        "        \"\"\"\n",
        "        m, n = len(source), len(target)\n",
        "\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "        operations = [[None] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "\n",
        "        for i in range(m + 1):\n",
        "            dp[i][0] = i * self.DELETE_COST\n",
        "            print(dp)\n",
        "            if i > 0:\n",
        "                operations[i][0] = ('DELETE', i-1, 0)\n",
        "\n",
        "        for j in range(n + 1):\n",
        "            dp[0][j] = j * self.INSERT_COST\n",
        "            if j > 0:\n",
        "                operations[0][j] = ('INSERT', 0, j-1)\n",
        "\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if source[i-1] == target[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1]\n",
        "                    operations[i][j] = ('COPY', i-1, j-1)\n",
        "                else:\n",
        "                    replace = dp[i-1][j-1] + self.REPLACE_COST\n",
        "                    delete = dp[i-1][j] + self.DELETE_COST\n",
        "                    insert = dp[i][j-1] + self.INSERT_COST\n",
        "\n",
        "                    min_cost = min(replace, delete, insert)\n",
        "                    dp[i][j] = min_cost\n",
        "\n",
        "                    if min_cost == replace:\n",
        "                        operations[i][j] = ('REPLACE', i-1, j-1)\n",
        "                    elif min_cost == delete:\n",
        "                        operations[i][j] = ('DELETE', i-1, j)\n",
        "                    else:\n",
        "                        operations[i][j] = ('INSERT', i, j-1)\n",
        "\n",
        "        print(dp)\n",
        "\n",
        "        edit_sequence = []\n",
        "        i, j = m, n\n",
        "\n",
        "        while i > 0 or j > 0:\n",
        "            operation, prev_i, prev_j = operations[i][j]\n",
        "\n",
        "            if operation == 'COPY':\n",
        "                edit_sequence.append(f\"Copy '{source[i-1]}'\")\n",
        "            elif operation == 'REPLACE':\n",
        "                edit_sequence.append(f\"Replace '{source[i-1]}' with '{target[j-1]}'\")\n",
        "            elif operation == 'DELETE':\n",
        "                edit_sequence.append(f\"Delete '{source[i-1]}'\")\n",
        "            else:\n",
        "                edit_sequence.append(f\"Insert '{target[j-1]}'\")\n",
        "\n",
        "            i, j = prev_i, prev_j\n",
        "\n",
        "        edit_sequence.reverse()\n",
        "\n",
        "        return dp[m][n], edit_sequence\n",
        "\n",
        "    def print_detailed_output(self, source: str, target: str):\n",
        "        \"\"\"Print detailed output including the edit distance and operations\"\"\"\n",
        "        distance, operations = self.minimum_edit_distance(source, target)\n",
        "\n",
        "        print(f\"Source string: {source}\")\n",
        "        print(f\"Target string: {target}\")\n",
        "        print(f\"Minimum Edit Distance: {distance}\")\n",
        "        print(\"\\nEdit Operations:\")\n",
        "        for i, op in enumerate(operations, 1):\n",
        "            print(f\"{i}. {op}\")\n",
        "\n",
        "\n",
        "def test_edit_distance():\n",
        "    ed = EditDistance()\n",
        "    ed.print_detailed_output(\"cat\", \"cut\")\n",
        "\n",
        "    # # Test cases\n",
        "    # test_cases = [\n",
        "    #     (\"kitten\", \"sitting\"),\n",
        "    #     (\"sunday\", \"saturday\"),\n",
        "    #     (\"intention\", \"execution\"),\n",
        "    #     (\"cat\", \"cut\"),\n",
        "    #     (\"\", \"hello\"),\n",
        "    #     (\"algorithm\", \"logarithm\"),\n",
        "    #     (\"hello\", \"hello\"),\n",
        "    # ]\n",
        "\n",
        "    # print(\"Testing Minimum Edit Distance Algorithm\")\n",
        "    # print(\"=\" * 50)\n",
        "\n",
        "    # for source, target in test_cases:\n",
        "    #     print(\"\\nTest Case:\")\n",
        "    #     print(\"-\" * 50)\n",
        "    #     ed.print_detailed_output(source, target)\n",
        "    #     print(\"-\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_edit_distance()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb5PkVsPlUJ4",
        "outputId": "e3e1334a-8d50-4dc3-8888-c715d5985346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0]]\n",
            "[[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0]]\n",
            "[[0, 1, 2, 3], [1, 0, 1, 2], [2, 1, 2, 3], [3, 2, 3, 2]]\n",
            "Source string: cat\n",
            "Target string: cut\n",
            "Minimum Edit Distance: 2\n",
            "\n",
            "Edit Operations:\n",
            "1. Copy 'c'\n",
            "2. Replace 'a' with 'u'\n",
            "3. Copy 't'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5-9CfHPnnJzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spell Checker - Needs Improvement"
      ],
      "metadata": {
        "id": "z8HZpPFoLDH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spell Checker"
      ],
      "metadata": {
        "id": "_DFqmcoHK7GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import words, brown, gutenberg\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('words')\n",
        "nltk.download('brown')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self):\n",
        "        self.vocabulary = set(words.words())\n",
        "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
        "        self.trigram_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "        self.unigram_counts = defaultdict(int)\n",
        "        self.word_freq = defaultdict(int)\n",
        "\n",
        "        common_words = {'misspelled', 'jumps', 'words', 'over'}\n",
        "        self.vocabulary.update(common_words)\n",
        "\n",
        "        self.train_model()\n",
        "\n",
        "    def train_model(self):\n",
        "        for corpus in [brown, gutenberg]:\n",
        "            for sentence in corpus.sents():\n",
        "                tokens = ['<s>', '<s>'] + [word.lower() for word in sentence] + ['</s>']\n",
        "\n",
        "                for token in tokens:\n",
        "                    self.word_freq[token.lower()] += 1\n",
        "\n",
        "                for i in range(len(tokens)-2):\n",
        "                    self.bigram_counts[tokens[i+1]][tokens[i+2]] += 1\n",
        "                    self.unigram_counts[tokens[i+1]] += 1\n",
        "\n",
        "                    self.trigram_counts[tokens[i]][tokens[i+1]][tokens[i+2]] += 1\n",
        "\n",
        "                self.unigram_counts[tokens[-1]] += 1\n",
        "\n",
        "    def get_edits1(self, word):\n",
        "        \"\"\"Generate all strings that are one edit distance away from the input word\"\"\"\n",
        "        letters = string.ascii_lowercase\n",
        "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "\n",
        "        deletes = [L + R[1:] for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "        inserts = [L + c + R for L, R in splits for c in letters]\n",
        "\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def get_edits2(self, word):\n",
        "        \"\"\"Generate all strings that are two edits away from the input word\"\"\"\n",
        "        return set(e2 for e1 in self.get_edits1(word) for e2 in self.get_edits1(e1))\n",
        "\n",
        "    def calculate_ngram_probability(self, prev_tokens, token):\n",
        "        \"\"\"Calculate probability using interpolated trigram/bigram/unigram models\"\"\"\n",
        "        lambda1, lambda2, lambda3 = 0.5, 0.3, 0.2\n",
        "\n",
        "        trigram_prob = 0\n",
        "        if len(prev_tokens) >= 2:\n",
        "            numerator = self.trigram_counts[prev_tokens[0]][prev_tokens[1]][token] + 1\n",
        "            denominator = self.bigram_counts[prev_tokens[0]][prev_tokens[1]] + len(self.vocabulary)\n",
        "            trigram_prob = numerator / denominator\n",
        "\n",
        "        bigram_prob = 0\n",
        "        if len(prev_tokens) >= 1:\n",
        "            numerator = self.bigram_counts[prev_tokens[-1]][token] + 1\n",
        "            denominator = self.unigram_counts[prev_tokens[-1]] + len(self.vocabulary)\n",
        "            bigram_prob = numerator / denominator\n",
        "\n",
        "        unigram_prob = (self.unigram_counts[token] + 1) / (sum(self.unigram_counts.values()) + len(self.vocabulary))\n",
        "\n",
        "        return lambda1 * trigram_prob + lambda2 * bigram_prob + lambda3 * unigram_prob\n",
        "\n",
        "    def score_candidate(self, candidate, prev_words, next_words):\n",
        "        \"\"\"Score a candidate word based on n-gram probability and word frequency\"\"\"\n",
        "        prev_tokens = ['<s>'] if not prev_words else prev_words[-2:]\n",
        "        prob_score = np.log(self.calculate_ngram_probability(prev_tokens, candidate))\n",
        "\n",
        "        if next_words:\n",
        "            next_prob = np.log(self.calculate_ngram_probability([prev_tokens[-1], candidate], next_words[0]))\n",
        "            prob_score += next_prob\n",
        "\n",
        "        freq_score = np.log(self.word_freq[candidate.lower()] + 1)\n",
        "\n",
        "        if prev_words and next_words:\n",
        "            avg_len = (len(prev_words[-1]) + len(next_words[0])) / 2\n",
        "            len_score = -abs(len(candidate) - avg_len) / 10\n",
        "        else:\n",
        "            len_score = 0\n",
        "\n",
        "        return prob_score + 0.5 * freq_score + len_score\n",
        "\n",
        "    def correct_word(self, word, prev_words, next_words):\n",
        "        \"\"\"Correct a single word using context\"\"\"\n",
        "        if word.lower() in self.vocabulary:\n",
        "            return word\n",
        "\n",
        "        # Generate candidates\n",
        "        candidates = self.get_edits1(word.lower())\n",
        "        valid_candidates = {c for c in candidates if c in self.vocabulary}\n",
        "\n",
        "        if not valid_candidates:\n",
        "            candidates2 = self.get_edits2(word.lower())\n",
        "            valid_candidates = {c for c in candidates2 if c in self.vocabulary}\n",
        "\n",
        "        if not valid_candidates:\n",
        "            return word\n",
        "\n",
        "        best_candidate = max(valid_candidates,\n",
        "                           key=lambda x: self.score_candidate(x, prev_words, next_words))\n",
        "\n",
        "        if word[0].isupper():\n",
        "            best_candidate = best_candidate.capitalize()\n",
        "\n",
        "        return best_candidate\n",
        "\n",
        "    def correct_text(self, text):\n",
        "        words = word_tokenize(text)\n",
        "        corrected_words = []\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            if word.isalpha():\n",
        "                prev_words = [w.lower() for w in words[max(0, i-2):i]]\n",
        "                next_words = [w.lower() for w in words[i+1:min(len(words), i+3)]]\n",
        "\n",
        "                corrected_word = self.correct_word(word, prev_words, next_words)\n",
        "                corrected_words.append(corrected_word)\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "\n",
        "        return \" \".join(corrected_words)\n",
        "\n",
        "def test_spell_checker():\n",
        "    spell_checker = SpellChecker()\n",
        "\n",
        "    test_cases = [\n",
        "        \"This is a test sentense with misspeled words\",\n",
        "        \"I recieved your mesage yestarday\",\n",
        "        \"The quick brwn fox jumps ovr the lasy dog\",\n",
        "        \"She was writting a letter to her frend\"\n",
        "    ]\n",
        "\n",
        "    print(\"Spell Checker Test Results:\")\n",
        "    print(\"-\" * 50)\n",
        "    for text in test_cases:\n",
        "        corrected_text = spell_checker.correct_text(text)\n",
        "        print(f\"Original:  {text}\")\n",
        "        print(f\"Corrected: {corrected_text}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing Improved Spell Checker:\")\n",
        "    test_spell_checker()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8mtk-B8jMRJ",
        "outputId": "bbf5f281-cd11-4adb-cc30-5c4d1e8df95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Improved Spell Checker:\n",
            "Spell Checker Test Results:\n",
            "--------------------------------------------------\n",
            "Original:  This is a test sentense with misspeled words\n",
            "Corrected: This is a test sentence with misspelled words\n",
            "--------------------------------------------------\n",
            "Original:  I recieved your mesage yestarday\n",
            "Corrected: I received your message yesterday\n",
            "--------------------------------------------------\n",
            "Original:  The quick brwn fox jumps ovr the lasy dog\n",
            "Corrected: The quick brown fox jumps or the last dog\n",
            "--------------------------------------------------\n",
            "Original:  She was writting a letter to her frend\n",
            "Corrected: She was writing a letter to her friend\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "LCye4jy8K3lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import movie_reviews"
      ],
      "metadata": {
        "id": "zcBecLkrTQbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "EhFDHOvpUAGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class NaiveBayesSentimentClassifier:\n",
        "    def __init__(self, k=1.0):\n",
        "        self.k = k  # Smoothing parameter\n",
        "        self.word_counts = {'pos': defaultdict(int), 'neg': defaultdict(int)}\n",
        "        self.class_counts = {'pos': 0, 'neg': 0}\n",
        "        self.vocabulary = set()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Preprocess the text by converting to lowercase and removing punctuation\"\"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(f'[{string.punctuation}]', '', text)\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\"Train the classifier on the given data\"\"\"\n",
        "        for text, label in zip(X_train, y_train):\n",
        "            words = self.preprocess(text)\n",
        "            self.class_counts[label] += 1\n",
        "\n",
        "            for word in words:\n",
        "                self.word_counts[label][word] += 1\n",
        "                self.vocabulary.add(word)\n",
        "\n",
        "    def calculate_probability(self, text, label):\n",
        "        \"\"\"Calculate P(text|label) using the Naive Bayes assumption\"\"\"\n",
        "        words = self.preprocess(text)\n",
        "        log_prob = np.log(self.class_counts[label] / sum(self.class_counts.values()))\n",
        "\n",
        "        vocab_size = len(self.vocabulary)\n",
        "        total_words = sum(self.word_counts[label].values())\n",
        "\n",
        "        for word in words:\n",
        "            count = self.word_counts[label].get(word, 0)\n",
        "            prob = (count + self.k) / (total_words + self.k * vocab_size)\n",
        "            log_prob += np.log(prob)\n",
        "\n",
        "        return log_prob\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"Predict the sentiment of the given text\"\"\"\n",
        "        pos_prob = self.calculate_probability(text, 'pos')\n",
        "        neg_prob = self.calculate_probability(text, 'neg')\n",
        "\n",
        "        return 'pos' if pos_prob > neg_prob else 'neg'\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluate the classifier on test data\"\"\"\n",
        "        correct = 0\n",
        "        total = len(X_test)\n",
        "\n",
        "        for text, true_label in zip(X_test, y_test):\n",
        "            pred_label = self.predict(text)\n",
        "            if pred_label == true_label:\n",
        "                correct += 1\n",
        "\n",
        "        return correct / total\n",
        "\n",
        "def test_sentiment_classifier():\n",
        "    documents = [(list(movie_reviews.words(fileid)), category)\n",
        "                for category in movie_reviews.categories()\n",
        "                for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "    np.random.shuffle(documents)\n",
        "\n",
        "    texts = [' '.join(doc) for doc, category in documents]\n",
        "    labels = [category for doc, category in documents]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    k_values = [0.25, 0.75, 1.0]\n",
        "\n",
        "    for k in k_values:\n",
        "        classifier = NaiveBayesSentimentClassifier(k=k)\n",
        "        classifier.train(X_train, y_train)\n",
        "        accuracy = classifier.evaluate(X_test, y_test)\n",
        "        print(f\"Accuracy with k={k}: {accuracy:.4f}\")\n",
        "\n",
        "test_sentiment_classifier()"
      ],
      "metadata": {
        "id": "aMum33MUkEXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1c370b-6779-4620-ef76-14cbdb4c3b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with k=0.25: 0.7975\n",
            "Accuracy with k=0.75: 0.8000\n",
            "Accuracy with k=1.0: 0.8050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Transition probabilities (a_ij)\n",
        "a_ij = {\n",
        "    'START': {'NN': 0.5, 'VB': 0.25, 'JJ': 0.25, 'RB': 0},\n",
        "    'NN': {'STOP': 0.25, 'NN': 0.25, 'VB': 0.5, 'JJ': 0, 'RB': 0},\n",
        "    'VB': {'STOP': 0.25, 'NN': 0.25, 'VB': 0, 'JJ': 0.25, 'RB': 0.25},\n",
        "    'JJ': {'STOP': 0, 'NN': 0.75, 'VB': 0, 'JJ': 0.25, 'RB': 0},\n",
        "    'RB': {'STOP': 0.5, 'NN': 0.25, 'VB': 0, 'JJ': 0.25, 'RB': 0},\n",
        "}\n",
        "\n",
        "# Emission probabilities (b_ik)\n",
        "b_ik = {\n",
        "    'NN': {'time': 0.1, 'flies': 0.01, 'fast': 0.01},\n",
        "    'VB': {'time': 0.01, 'flies': 0.1, 'fast': 0.01},\n",
        "    'JJ': {'time': 0, 'flies': 0, 'fast': 0.1},\n",
        "    'RB': {'time': 0, 'flies': 0, 'fast': 0.1},\n",
        "}\n",
        "\n",
        "# POS Tags and Words\n",
        "states = ['NN', 'VB', 'JJ', 'RB']\n",
        "sentence = ['time', 'flies', 'fast']\n",
        "\n",
        "# Viterbi Algorithm Implementation\n",
        "def viterbi(sentence, states, start_prob, transition_prob, emission_prob):\n",
        "    T = len(sentence)\n",
        "    N = len(states)\n",
        "\n",
        "    # Initialization\n",
        "    V = np.zeros((T, N))  # Viterbi matrix\n",
        "    backpointer = np.zeros((T, N), dtype=int)  # Backpointer to reconstruct path\n",
        "\n",
        "    # Initialize with START probabilities\n",
        "    for i, state in enumerate(states):\n",
        "        V[0, i] = start_prob[state] * emission_prob[state].get(sentence[0], 0)\n",
        "\n",
        "    # Recursion step\n",
        "    for t in range(1, T):\n",
        "        for j, state_j in enumerate(states):\n",
        "            max_prob, max_state = 0, 0\n",
        "            for i, state_i in enumerate(states):\n",
        "                prob = V[t-1, i] * transition_prob[state_i].get(state_j, 0) * emission_prob[state_j].get(sentence[t], 0)\n",
        "                if prob > max_prob:\n",
        "                    max_prob, max_state = prob, i\n",
        "            V[t, j] = max_prob\n",
        "            backpointer[t, j] = max_state\n",
        "\n",
        "    # Termination: Transition to STOP\n",
        "    final_probs = [V[T-1, i] * transition_prob[states[i]].get('STOP', 0) for i in range(N)]\n",
        "    best_final_state = np.argmax(final_probs)\n",
        "\n",
        "    # Backtracking\n",
        "    best_path = [best_final_state]\n",
        "    for t in range(T-1, 0, -1):\n",
        "        best_path.insert(0, backpointer[t, best_path[0]])\n",
        "\n",
        "    # Convert state indices to state names\n",
        "    best_path_states = [states[state] for state in best_path]\n",
        "    return best_path_states, max(final_probs)\n",
        "\n",
        "# Start probabilities from START\n",
        "def get_start_prob(states):\n",
        "    return {state: a_ij['START'].get(state, 0) for state in states}\n",
        "\n",
        "# Run Viterbi\n",
        "start_prob = get_start_prob(states)\n",
        "most_likely_tags, max_prob = viterbi(sentence, states, start_prob, a_ij, b_ik)\n",
        "\n",
        "# Output Results\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Most Likely POS Tags:\", most_likely_tags)\n",
        "print(\"Probability of Best Path:\", max_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36KydCbo01uy",
        "outputId": "a66a7bf9-7d1c-41eb-a9aa-3a50ea304a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: ['time', 'flies', 'fast']\n",
            "Most Likely POS Tags: ['NN', 'VB', 'RB']\n",
            "Probability of Best Path: 3.125000000000001e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# Function to preprocess text (simple tokenization)\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # Lowercase the text\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation\n",
        "    return text.split()\n",
        "\n",
        "# Function to calculate bigram probabilities\n",
        "def calculate_bigrams(corpus):\n",
        "    unigram_counts = defaultdict(int)\n",
        "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Tokenize corpus and calculate counts\n",
        "    for sentence in corpus:\n",
        "        tokens = ['<s>'] + preprocess(sentence) + ['</s>']\n",
        "        for i in range(len(tokens) - 1):\n",
        "            unigram_counts[tokens[i]] += 1\n",
        "            bigram_counts[tokens[i]][tokens[i + 1]] += 1\n",
        "        unigram_counts[tokens[-1]] += 1  # Count </s> as a unigram\n",
        "\n",
        "    # Calculate bigram probabilities\n",
        "    bigram_probs = defaultdict(dict)\n",
        "    for w1 in bigram_counts:\n",
        "        for w2 in bigram_counts[w1]:\n",
        "            bigram_probs[w1][w2] = bigram_counts[w1][w2] / unigram_counts[w1]\n",
        "\n",
        "    return bigram_probs\n",
        "\n",
        "# Function to calculate sentence probability using bigram model\n",
        "def sentence_probability(sentence, bigram_probs):\n",
        "    tokens = ['<s>'] + preprocess(sentence) + ['</s>']\n",
        "    prob = 1.0\n",
        "\n",
        "    for i in range(len(tokens) - 1):\n",
        "        w1, w2 = tokens[i], tokens[i + 1]\n",
        "        if w2 in bigram_probs.get(w1, {}):\n",
        "            prob *= bigram_probs[w1][w2]\n",
        "        else:\n",
        "            prob *= 0  # If bigram doesn't exist, probability is 0\n",
        "\n",
        "    return prob\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    corpus = [\n",
        "        \"The cat sat on the mat\",\n",
        "        \"The cat ate the mouse\",\n",
        "        \"The dog barked loudly\"\n",
        "    ]\n",
        "\n",
        "    # Calculate bigram probabilities\n",
        "    bigram_probs = calculate_bigrams(corpus)\n",
        "\n",
        "    # Print bigram probabilities\n",
        "    print(\"Bigram Probabilities:\")\n",
        "    for w1 in bigram_probs:\n",
        "        for w2 in bigram_probs[w1]:\n",
        "            print(f\"P({w2} | {w1}) = {bigram_probs[w1][w2]:.4f}\")\n",
        "\n",
        "    # Test sentence probability\n",
        "    test_sentence = \"The cat sat\"\n",
        "    prob = sentence_probability(test_sentence, bigram_probs)\n",
        "    print(f\"\\nProbability of sentence '{test_sentence}': {prob:.8f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFrw4B9x06WS",
        "outputId": "87a2ea36-e857-4e71-cc65-b2da1b094dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Probabilities:\n",
            "P(the | <s>) = 1.0000\n",
            "P(cat | the) = 0.4000\n",
            "P(mat | the) = 0.2000\n",
            "P(mouse | the) = 0.2000\n",
            "P(dog | the) = 0.2000\n",
            "P(sat | cat) = 0.5000\n",
            "P(ate | cat) = 0.5000\n",
            "P(on | sat) = 1.0000\n",
            "P(the | on) = 1.0000\n",
            "P(</s> | mat) = 1.0000\n",
            "P(the | ate) = 1.0000\n",
            "P(</s> | mouse) = 1.0000\n",
            "P(barked | dog) = 1.0000\n",
            "P(loudly | barked) = 1.0000\n",
            "P(</s> | loudly) = 1.0000\n",
            "\n",
            "Probability of sentence 'The cat sat': 0.00000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to compute TF-IDF matrix\n",
        "def compute_tfidf(documents):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    return tfidf_matrix, feature_names, vectorizer\n",
        "\n",
        "# Function to calculate cosine similarity between two documents\n",
        "def cosine_similarity_docs(tfidf_matrix, doc_index1, doc_index2):\n",
        "    return cosine_similarity(tfidf_matrix[doc_index1], tfidf_matrix[doc_index2])[0][0]\n",
        "\n",
        "# Function to calculate cosine similarity between two words\n",
        "def cosine_similarity_words(word1, word2, vectorizer, feature_names, tfidf_matrix):\n",
        "    if word1 not in feature_names or word2 not in feature_names:\n",
        "        return 0.0\n",
        "\n",
        "    word_index1 = np.where(feature_names == word1)[0][0]\n",
        "    word_index2 = np.where(feature_names == word2)[0][0]\n",
        "\n",
        "    word_vector1 = tfidf_matrix[:, word_index1].toarray().flatten()\n",
        "    word_vector2 = tfidf_matrix[:, word_index2].toarray().flatten()\n",
        "\n",
        "    return cosine_similarity(word_vector1.reshape(1, -1), word_vector2.reshape(1, -1))[0][0]\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    documents = [\n",
        "        \"The cat sat on the mat\",\n",
        "        \"The dog barked at the cat\",\n",
        "        \"The mouse ran across the room\",\n",
        "        \"The cat chased the mouse\"\n",
        "    ]\n",
        "\n",
        "    # Compute TF-IDF matrix\n",
        "    tfidf_matrix, feature_names, vectorizer = compute_tfidf(documents)\n",
        "    print(\"TF-IDF Matrix:\")\n",
        "    print(tfidf_matrix.toarray())\n",
        "    print(\"\\nFeature Names:\")\n",
        "    print(feature_names)\n",
        "\n",
        "    # Calculate cosine similarity between documents\n",
        "    doc_index1 = 0  # First document\n",
        "    doc_index2 = 1  # Second document\n",
        "    similarity = cosine_similarity_docs(tfidf_matrix, doc_index1, doc_index2)\n",
        "    print(f\"\\nCosine Similarity between Document {doc_index1} and Document {doc_index2}: {similarity:.4f}\")\n",
        "\n",
        "    # Calculate cosine similarity between words\n",
        "    word1 = \"cat\"\n",
        "    word2 = \"dog\"\n",
        "    word_similarity = cosine_similarity_words(word1, word2, vectorizer, feature_names, tfidf_matrix)\n",
        "    print(f\"\\nCosine Similarity between words '{word1}' and '{word2}': {word_similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXSd5fXN1mOk",
        "outputId": "731c79af-8d5a-4de8-8456-3b017b8818bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.30100231 0.         0.\n",
            "  0.47157828 0.         0.47157828 0.         0.         0.47157828\n",
            "  0.49217822]\n",
            " [0.         0.47157828 0.47157828 0.30100231 0.         0.47157828\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.49217822]\n",
            " [0.46073328 0.         0.         0.         0.         0.\n",
            "  0.         0.36324741 0.         0.46073328 0.46073328 0.\n",
            "  0.48085948]\n",
            " [0.         0.         0.         0.36145869 0.56629489 0.\n",
            "  0.         0.4464734  0.         0.         0.         0.\n",
            "  0.59103233]]\n",
            "\n",
            "Feature Names:\n",
            "['across' 'at' 'barked' 'cat' 'chased' 'dog' 'mat' 'mouse' 'on' 'ran'\n",
            " 'room' 'sat' 'the']\n",
            "\n",
            "Cosine Similarity between Document 0 and Document 1: 0.3328\n",
            "\n",
            "Cosine Similarity between words 'cat' and 'dog': 0.5390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter, defaultdict\n",
        "import itertools\n",
        "\n",
        "# Function to preprocess text (simple tokenization)\n",
        "def preprocess(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Function to build co-occurrence matrix\n",
        "def build_cooccurrence_matrix(corpus, window_size=2):\n",
        "    vocabulary = set()\n",
        "    word_counts = Counter()\n",
        "    cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Tokenize sentences and build counts\n",
        "    for sentence in corpus:\n",
        "        tokens = preprocess(sentence)\n",
        "        vocabulary.update(tokens)\n",
        "        word_counts.update(tokens)\n",
        "\n",
        "        for i, word in enumerate(tokens):\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(tokens), i + window_size + 1)\n",
        "            for j in range(start, end):\n",
        "                if i != j:\n",
        "                    cooccurrence_counts[word][tokens[j]] += 1\n",
        "\n",
        "    vocabulary = sorted(vocabulary)\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "    cooccurrence_matrix = np.zeros((len(vocabulary), len(vocabulary)))\n",
        "\n",
        "    for word, neighbors in cooccurrence_counts.items():\n",
        "        for neighbor, count in neighbors.items():\n",
        "            cooccurrence_matrix[word_to_index[word]][word_to_index[neighbor]] = count\n",
        "\n",
        "    return cooccurrence_matrix, word_to_index, vocabulary\n",
        "\n",
        "# Function to compute PPMI matrix\n",
        "def compute_ppmi_matrix(cooccurrence_matrix):\n",
        "    total_sum = np.sum(cooccurrence_matrix)\n",
        "    word_sum = np.sum(cooccurrence_matrix, axis=1)\n",
        "    context_sum = np.sum(cooccurrence_matrix, axis=0)\n",
        "\n",
        "    ppmi_matrix = np.zeros_like(cooccurrence_matrix)\n",
        "    for i in range(cooccurrence_matrix.shape[0]):\n",
        "        for j in range(cooccurrence_matrix.shape[1]):\n",
        "            p_wc = cooccurrence_matrix[i][j] / total_sum\n",
        "            p_w = word_sum[i] / total_sum\n",
        "            p_c = context_sum[j] / total_sum\n",
        "\n",
        "            if p_wc > 0:\n",
        "                ppmi = max(0, np.log2(p_wc / (p_w * p_c)))\n",
        "                ppmi_matrix[i][j] = ppmi\n",
        "\n",
        "    return ppmi_matrix\n",
        "\n",
        "# Function to compute cosine similarity between two vectors\n",
        "def cosine_similarity_vectors(vec1, vec2):\n",
        "    return cosine_similarity(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0]\n",
        "\n",
        "# Function to compute cosine similarity between two words\n",
        "def cosine_similarity_words(word1, word2, ppmi_matrix, word_to_index):\n",
        "    if word1 not in word_to_index or word2 not in word_to_index:\n",
        "        return 0.0\n",
        "    index1 = word_to_index[word1]\n",
        "    index2 = word_to_index[word2]\n",
        "    return cosine_similarity_vectors(ppmi_matrix[index1], ppmi_matrix[index2])\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    corpus = [\n",
        "        \"the cat sat on the mat\",\n",
        "        \"the dog barked at the cat\",\n",
        "        \"the mouse ran across the room\",\n",
        "        \"the cat chased the mouse\"\n",
        "    ]\n",
        "\n",
        "    # Build co-occurrence and PPMI matrices\n",
        "    cooccurrence_matrix, word_to_index, vocabulary = build_cooccurrence_matrix(corpus, window_size=2)\n",
        "    ppmi_matrix = compute_ppmi_matrix(cooccurrence_matrix)\n",
        "\n",
        "    print(\"Vocabulary:\")\n",
        "    print(vocabulary)\n",
        "    # print(\"\\nPPMI Matrix:\")\n",
        "    # print(ppmi_matrix)\n",
        "\n",
        "    # Cosine similarity between words\n",
        "    word1 = \"cat\"\n",
        "    word2 = \"dog\"\n",
        "    similarity = cosine_similarity_words(word1, word2, ppmi_matrix, word_to_index)\n",
        "    print(f\"\\nCosine Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n",
        "\n",
        "    # Cosine similarity between two documents\n",
        "    doc1_vector = np.sum(ppmi_matrix, axis=0)  # Sum word vectors for a document\n",
        "    doc2_vector = np.sum(ppmi_matrix, axis=1)  # Sum word vectors for a second document\n",
        "    doc_similarity = cosine_similarity_vectors(doc1_vector, doc2_vector)\n",
        "    print(f\"\\nCosine Similarity between Document 1 and Document 2: {doc_similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqr1Hqup15fW",
        "outputId": "4cccc57f-d327-4545-bcb3-b8a2c7b4718b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['across', 'at', 'barked', 'cat', 'chased', 'dog', 'mat', 'mouse', 'on', 'ran', 'room', 'sat', 'the']\n",
            "\n",
            "Cosine Similarity between 'cat' and 'dog': 0.3500\n",
            "\n",
            "Cosine Similarity between Document 1 and Document 2: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "# Training Data: Provided sentences with sense labels\n",
        "train_data = [\n",
        "    (\"I love fish. The smoked bass fish was delicious.\", \"fish\"),\n",
        "    (\"The bass fish swam along the line.\", \"fish\"),\n",
        "    (\"He hauled in a big catch of smoked bass fish.\", \"fish\"),\n",
        "    (\"The bass guitar player played a smooth jazz line.\", \"guitar\"),\n",
        "]\n",
        "\n",
        "# Preprocess the sentences: tokenize and clean\n",
        "def preprocess(sentence):\n",
        "    return re.findall(r'\\b\\w+\\b', sentence.lower())  # Lowercase and tokenize\n",
        "\n",
        "# Build vocabulary and class-wise word counts\n",
        "def train_naive_bayes(train_data):\n",
        "    vocab = set()\n",
        "    word_counts = defaultdict(Counter)  # Word counts per class\n",
        "    class_counts = defaultdict(int)     # Count of sentences per class\n",
        "\n",
        "    for sentence, label in train_data:\n",
        "        tokens = preprocess(sentence)\n",
        "        vocab.update(tokens)\n",
        "        word_counts[label].update(tokens)\n",
        "        class_counts[label] += 1\n",
        "\n",
        "    return vocab, word_counts, class_counts\n",
        "\n",
        "# Calculate log probabilities using Add-1 Smoothing\n",
        "def calculate_log_probabilities(vocab, word_counts, class_counts):\n",
        "    total_classes = sum(class_counts.values())\n",
        "    log_probs = {}\n",
        "    total_vocab_size = len(vocab)\n",
        "\n",
        "    for label in class_counts:\n",
        "        log_probs[label] = {\n",
        "            'class_log_prob': math.log(class_counts[label] / total_classes),\n",
        "            'word_log_probs': {}\n",
        "        }\n",
        "        total_words = sum(word_counts[label].values())\n",
        "\n",
        "        for word in vocab:\n",
        "            word_freq = word_counts[label][word] + 1  # Add-1 smoothing\n",
        "            log_probs[label]['word_log_probs'][word] = math.log(word_freq / (total_words + total_vocab_size))\n",
        "\n",
        "    return log_probs\n",
        "\n",
        "# Predict the sense of the test word based on the test sentence\n",
        "def predict(test_sentence, target_word, log_probs, vocab):\n",
        "    tokens = preprocess(test_sentence)\n",
        "    best_label = None\n",
        "    best_log_prob = float('-inf')\n",
        "\n",
        "    # Check probabilities for each class\n",
        "    for label in log_probs:\n",
        "        total_log_prob = log_probs[label]['class_log_prob']\n",
        "        for word in tokens:\n",
        "            if word in vocab:\n",
        "                total_log_prob += log_probs[label]['word_log_probs'].get(word, 0)\n",
        "\n",
        "        # Update best label if higher probability is found\n",
        "        if total_log_prob > best_log_prob:\n",
        "            best_log_prob = total_log_prob\n",
        "            best_label = label\n",
        "\n",
        "    return best_label\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Train the Naive Bayes model\n",
        "    vocab, word_counts, class_counts = train_naive_bayes(train_data)\n",
        "    log_probs = calculate_log_probabilities(vocab, word_counts, class_counts)\n",
        "\n",
        "    # Test sentence\n",
        "    test_sentence = \"He loves jazz. The bass line provided the foundation for the guitar solo in the jazz piece\"\n",
        "    test_word = \"bass\"\n",
        "\n",
        "    # Predict the sense of 'bass'\n",
        "    predicted_sense = predict(test_sentence, test_word, log_probs, vocab)\n",
        "    print(f\"Test sentence: {test_sentence}\")\n",
        "    print(f\"Test word: {test_word}\")\n",
        "    print(f\"Output: {predicted_sense}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsZD1fZc2Xpg",
        "outputId": "8e8445e2-d22d-4b16-af86-f72f23fe24bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test sentence: He loves jazz. The bass line provided the foundation for the guitar solo in the jazz piece\n",
            "Test word: bass\n",
            "Output: guitar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kSRzJ5eF2vyM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}